import pylayer as L
import cupy as cp

'''
    utility: compute two "2-d" lists with weight using operator op
    a = a * wa op b * wb
'''


def list2d_add(a, wa, b, wb):
    for x, y in zip(a, b):
        # print(len(x), len(y))
        for u, v in zip(x, y):
            # print('\t', u.shape, v.shape)
            u *= wa
            u += v * wb


def list2d_sub(a, wa, b, wb):
    for x, y in zip(a, b):
        # print(len(x), len(y))
        for u, v in zip(x, y):
            # print('\t', u.shape, v.shape)
            u *= wa
            u -= v * wb


def layers(model):
    params_ref = []
    for layer in model.layers:
        # crappy implementation, not my bad
        if isinstance(layer, L.Linear):
            params_ref.append((layer.weight, layer.bias))
        elif isinstance(layer, L.BatchNorm1d):
            params_ref.append((layer.gamma, layer.beta))
        elif isinstance(layer, L.BatchNorm2d):
            params_ref.append((layer.gamma, layer.beta))
        elif isinstance(layer, L.ReLU):
            params_ref.append(())
        elif isinstance(layer, L.ReLU6):
            params_ref.append(())
        elif isinstance(layer, L.CrossEntropyLossWithSoftmax):
            params_ref.append(())
        elif isinstance(layer, L.Conv2d):
            params_ref.append((layer.weight, layer.bias))
        elif isinstance(layer, L.MaxPool2d):
            params_ref.append(())
        elif isinstance(layer, L.AvgPool2d):
            params_ref.append(())
        elif isinstance(layer, L.Flatten):
            params_ref.append(())
        elif isinstance(layer, L.BasicBlock):
            params_ref.append(tuple(layer.params_ref))
        elif isinstance(layer, L.BottleNeck):
            params_ref.append(tuple(layer.params_ref))
    return params_ref


class SGD(object):
    """
        Implements stochastic gradient descent, with momentum features.

        The model could be sequential, must have forward(), backward();
        model.param_grads is generated by model.backward().
    """

    def __init__(self, model, lr=1e-3, momentum=0.):
        self.lr = lr
        self.momentum = momentum
        self.last_step_grads = None
        self.model = model
        # maintains a reference of all trainable params of all layers
        self.params_ref = layers(self.model)

    def step(self):
        if self.momentum > 0. and self.last_step_grads is not None:
            list2d_add(self.model.param_grads, 1. - self.momentum, self.last_step_grads, self.momentum)
        list2d_sub(self.params_ref, 1., self.model.param_grads, self.lr)
        self.last_step_grads = self.model.param_grads.copy()


class AdaGrad(object):
    def __init__(self, model, lr=0.01):
        self.lr = lr
        self.grad2_sum = None
        self.last_step_grads = None
        self.model = model
        # maintains a reference of all trainable params of all layers
        self.params_ref = layers(self.model)

    def step(self):
        if self.grad2_sum is None:
            self.grad2_sum = [[0 for _ in range(len(self.model.param_grads[0]))]
                              for _ in range(len(self.model.param_grads))]
            list2d_sub(self.params_ref, 1., self.model.param_grads, self.lr)
        if self.last_step_grads is not None:
            grads_square = [[x ** 2 for x in row] for row in self.last_step_grads]
            list2d_add(self.grad2_sum, 1, grads_square, 1)
            self._update(self.params_ref, self.grad2_sum, self.model.param_grads, self.lr)
        self.last_step_grads = self.model.param_grads.copy()

    @staticmethod
    def _update(para, grad2_sum, grad, lr, eps=1e-6):
        for x, y, z in zip(para, grad2_sum, grad):
            for u, v, t in zip(x, y, z):
                u -= lr / cp.sqrt(v + eps) * t
